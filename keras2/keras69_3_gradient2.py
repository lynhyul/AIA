import numpy

f = lambda x : x**2 - 4*x + 6 

gradient = lambda x : 2*x -4

x0 = 10
epoch = 200
learning_rate = 0.1

print("step\tx\tf(x)")
print("{:02d}\t{:6.5f}\t{:6.5f}".format(0, x0,f(x0)))

for i in range(epoch):
    temp = x0 - learning_rate *gradient(x0)
    x0 = temp

    print("{:02d}\t{:6.5f}\t{:6.5f}".format(i+1, x0,f(x0)))

'''
00      10.00000        66.00000
01      8.40000 42.96000
02      7.12000 28.21440
03      6.09600 18.77722
04      5.27680 12.73742
05      4.62144 8.87195
06      4.09715 6.39805
07      3.67772 4.81475
08      3.34218 3.80144
09      3.07374 3.15292
10      2.85899 2.73787
11      2.68719 2.47224
12      2.54976 2.30223
13      2.43980 2.19343
14      2.35184 2.12379
15      2.28147 2.07923
16      2.22518 2.05071
17      2.18014 2.03245
18      2.14412 2.02077
19      2.11529 2.01329
20      2.09223 2.00851
21      2.07379 2.00544
22      2.05903 2.00348
23      2.04722 2.00223
24      2.03778 2.00143
25      2.03022 2.00091
26      2.02418 2.00058
27      2.01934 2.00037
28      2.01547 2.00024
29      2.01238 2.00015
30      2.00990 2.00010
'''